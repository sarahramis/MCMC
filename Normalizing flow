import jax
import jax.numpy as jnp
from jax import random
from flax import linen as nn
from jax.scipy.special import logsumexp
from jax import random, vmap
from jax.scipy.stats import norm
import numpy as np
from jax import grad
from jax import jit, value_and_grad
import optax


def rescorla_wagner(V_initial, alpha, beta, stimuli, rewards):
    """
    Simulate the Rescorla-Wagner model.
    :param V_initial: Initial value estimate.
    :param alpha: Learning rate for the prediction error.
    :param beta: Learning rate for the stimulus.
    :param stimuli: Array of stimuli presented (0 or 1).
    :param rewards: Array of received rewards.
    :return: Array of predicted values.
    """
    V = V_initial
    predictions = []
    for stimulus, reward in zip(stimuli, rewards):
        prediction = V * stimulus
        predictions.append(prediction)
        V += alpha * beta * (reward - prediction) * stimulus
    return jnp.array(predictions)

def simulate_rescorla_wagner(trials, alpha, beta, p_reward=0.5, initial_value=0.0, noise_level=0.0):
    """
    Simulate the Rescorla-Wagner model with enhancements.

    Parameters:
    - trials: int, number of trials to simulate.
    - alpha: float, learning rate for the prediction error.
    - beta: float, influence rate of the stimulus.
    - p_reward: float, probability of receiving a reward (dynamic or static).
    - initial_value: float, starting value for V.
    - noise_level: float, standard deviation of Gaussian noise added to each update.

    Returns:
    - V: np.array, values learned over trials.
    - X: np.array, presence of stimuli in each trial.
    - R: np.array, rewards received in each trial.
    """
    V = np.full(trials, initial_value)  # Initial values array
    X = np.random.choice([0, 1], size=trials)  # Stimuli presence
    R = np.random.binomial(1, p_reward, size=trials)  # Rewards based on given probability

    for t in range(1, trials):
        # Update rule with noise
        prediction_error = (R[t-1] - V[t-1]) * X[t-1]
        update = alpha * beta * prediction_error
        noise = np.random.normal(0, noise_level)  # Gaussian noise
        V[t] = V[t-1] + update + noise

    return V, X, R

class MaskedDense(nn.Module):
    features: int
    out_features: int
    kernel_init: Callable[..., Any] = nn.initializers.lecun_normal()
    bias_init: Callable[..., Any] = nn.initializers.zeros

    def setup(self):
        self.mask = jnp.tril(jnp.ones((self.features, self.out_features)), -1)

    @nn.compact
    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:
        print("Input shape to MaskedDense:", inputs.shape)  # Debugging line
        kernel = self.param('kernel', self.kernel_init, (self.features, self.out_features))
        bias = self.param('bias', self.bias_init, (self.out_features,))
        masked_kernel = jnp.multiply(self.mask, kernel)
        outputs = jnp.dot(inputs, masked_kernel) + bias
        print(f"Output shape in MaskedDense: {outputs.shape}")  # Debugging line
        return outputs
class TwoParamMAF(nn.Module):
    features: int
    hidden_units: int
    num_blocks: int

    @nn.compact
    def __call__(self, x):
        key = self.make_rng('noise')
        print("Initial input x shape:", x.shape)

        
        print(f"Input shape: {x.shape}")  # Debugging statement
        for i in range(self.num_blocks):
            # print(f"Input shape to MaskedDense: {x.shape}")
            # x = nn.leaky_relu(MaskedDense(features=x.shape[-1], out_features=self.hidden_units)(x), negative_slope=0.01)
            # print(f"Shape after block {i + 1}: {x.shape}")  # Check shape
            x = nn.leaky_relu(MaskedDense(features=x.shape[-1], out_features=self.hidden_units)(x), negative_slope=0.01)
            print(f"Shape after block {i + 1}: {x.shape}")

        log_scale = MaskedDense(self.hidden_units, self.features)(x)
        shift = MaskedDense(self.hidden_units, self.features)(x)
        # shift = MaskedDense(self.hidden_units, self.features)(x)
        print("Shift shape:", shift.shape)  # Should be (batch_size, num_features) with num_features >= 2

        if shift.shape[-1] < 2:
            raise ValueError("Shift output must have at least two features for alpha and beta computations.")

        # log_scale = MaskedDense(self.hidden_units, self.features)(x)
        # shift = MaskedDense(self.hidden_units, self.features)(x)
        print("Log scale shape:", log_scale.shape)
        print("Shift shape:", shift.shape)

        z = shift * jnp.exp(log_scale) + random.normal(key, shift.shape) * 0.1
        print("Z shape:", z.shape)

        # Ensure element-wise transformation
        alpha = nn.sigmoid(z[..., 0])  # Assuming the first column is for alpha
        beta = nn.softplus(z [..., 1])  # Assuming the second column is for beta

        print("Alpha shape:", alpha.shape)
        print("Beta shape:", beta.shape)
        print("Alpha shape:", alpha.shape)
        if alpha.ndim == 0:
            raise ValueError("Alpha is a scalar, expected at least a 1D array.")

  
        print("Key type:", type(key))

        log_det_jacobian = jnp.sum(log_scale, axis=-1)
        rng_keys = random.split(key, num=alpha.shape[0])  # Split keys for each alpha/beta pair
        # def draw_beta_samples(key, alpha, beta):
        #     return random.beta(key, alpha, beta, shape=(100,))

        # # Vectorize this function across batches
        # draw_beta_samples_batched = vmap(draw_beta_samples, in_axes=(0, 0, 0))

        # # Now apply this vectorized function
        # samples = draw_beta_samples_batched(rng_keys, alpha, beta)
        samples = vmap(lambda k, a, b: random.beta(k, a, b, shape=(100,)), in_axes=(0, 0, 0))(rng_keys, alpha, beta)
        # samples = draw_beta_samples_batched(rng_keys, alpha, beta, 100)
        

    
        return samples, alpha, beta, log_det_jacobian

def data_loader(X, R, batch_size):
    num_samples = X.shape[0]
    for i in range(0, num_samples, batch_size):
        X_batch = X[i:i + batch_size]
        R_batch = R[i:i + batch_size]
        if R_batch.ndim == 1:
            R_batch = R_batch[:, None]  # Ensure R_batch is always 2-dimensional for consistency
        yield X_batch, R_batch

def draw_beta_samples(key, alpha, beta, num_samples=100):
    # Returns samples of shape (num_samples,) for each alpha/beta pair
    return random.beta(key, alpha, beta, shape=(num_samples,))
draw_beta_samples_batched = vmap(draw_beta_samples, in_axes=(0, 0, 0, None))

def train_maf(rng, model, X, R, epochs=30):
    init_rng, noise_rng = random.split(rng)
    params = model.init({'params': init_rng, 'noise': noise_rng}, X)
    optimizer = optax.adam(learning_rate=0.0000001)
    opt_state = optimizer.init(params)

    def loss_fn(params, rng, X, R):
        _, noise_rng = random.split(rng)
        samples, alpha, beta, log_det_jacobian = model.apply(params, X, rngs={'noise': noise_rng})

        # Compute the negative log-likelihood as the loss
        R = jnp.clip(R, 1e-6, 1 - 1e-6)  # Avoid log(0)
        log_likelihood = (alpha - 1) * jnp.log(R) + (beta - 1) * jnp.log(1 - R) - jax.scipy.special.betaln(alpha, beta)
        loss = -jnp.mean(log_likelihood)

        return loss, log_det_jacobian 
    @jit
    def step(rng, params, opt_state, X, R):
        grads = grad(loss_fn)(params, rng, X, R)
        updates, opt_state = optimizer.update(grads, opt_state, params)
        params = optax.apply_updates(params, updates)
        return params, opt_state
    grad_fn = value_and_grad(loss_fn, has_aux=True)
    for epoch in range(epochs):
        for X_batch, R_batch in data_loader(X, R, 1000):
            rng, step_rng = random.split(rng)
            (loss, aux_output), grads = grad_fn(params, step_rng, X_batch, R_batch)  # Note how the outputs are unpacked
            updates, opt_state = optimizer.update(grads, opt_state, params)
            params = optax.apply_updates(params, updates)
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss}, Aux: {aux_output}")

    return params

trials = 10000
alpha = 0.3
beta = 0.5
p_reward = 0.5  # Constant probability of reward
initial_value = 0.0  # Start learning from no prior knowledge
noise_level = 0.01  # Some noise to simulate variability in learning
batch_size=1000

V, X, R = simulate_rescorla_wagner(trials, alpha, beta, p_reward, initial_value, noise_level)

# You might want to plot V to see how it evolves
import matplotlib.pyplot as plt
plt.plot(V)
plt.title('Learned Values Over Trials')
plt.xlabel('Trial')
plt.ylabel('Value')
plt.show()

X = X.reshape(-1, 2)  
R = R.reshape(-1, 1) 

rng = random.PRNGKey(0)
model = TwoParamMAF(features=2, hidden_units=10, num_blocks=3)
trained_params = train_maf(rng, model, X, R, epochs=30)

dummy_input = jnp.zeros((5, model.features))  # Ensure this matches the 'features' size expected by the model

# Make sure to use a new split or a properly managed RNG key for application
_, noise_rng = random.split(rng)  # Ensure you are using a fresh split for application
samples, estimated_alpha, estimated_beta, log_det_jacobian = model.apply(
    trained_params, dummy_input, rngs={'noise': noise_rng}
)


print("True alpha:", alpha, "Estimated alpha:", estimated_alpha)
print("True beta:", beta, "Estimated beta:", estimated_beta)

# Quantitative comparison
mse_alpha = np.mean((alpha - estimated_alpha) ** 2)
mse_beta = np.mean((beta - estimated_beta) ** 2)
print("MSE Alpha:", mse_alpha, "MSE Beta:", mse_beta)
